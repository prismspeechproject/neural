{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 64\n",
    "num_samples = 127606\n",
    "filename = 'English_hindi.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-361248ad937a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtarget_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_characters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "# vectorize the data\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = []\n",
    "target_characters = []\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "input_texts = df['English'].values.tolist()\n",
    "target_texts = df['Hindi'].values.tolist()\n",
    "\n",
    "for i in range(len(target_texts)):\n",
    "    target_texts[i] = '\\t' + target_texts[i] + '\\n'\n",
    "    for char in input_texts[i]:\n",
    "        if char not in input_characters:\n",
    "            input_characters.append(char)\n",
    "    for char in target_texts[i]:\n",
    "        if char not in target_characters:\n",
    "            target_characters.append(char)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_len = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_len = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('i/p characters : {}'.format(len(input_characters)))\n",
    "print('targert characters : {}'.format(len(target_characters)))\n",
    "print('encoder characters : {}'.format(num_encoder_tokens))\n",
    "print('decoder characters : {}'.format(num_decoder_tokens))\n",
    "print('max encoder : {}'.format(max_encoder_seq_len))\n",
    "print('max decoder : {}'.format(max_decoder_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, '\"': 2, '$': 3, \"'\": 4, ',': 5, '-': 6, '.': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, ':': 18, '?': 19, 'A': 20, 'B': 21, 'C': 22, 'D': 23, 'E': 24, 'F': 25, 'G': 26, 'H': 27, 'I': 28, 'J': 29, 'K': 30, 'L': 31, 'M': 32, 'N': 33, 'O': 34, 'P': 35, 'Q': 36, 'R': 37, 'S': 38, 'T': 39, 'U': 40, 'V': 41, 'W': 42, 'Y': 43, 'a': 44, 'b': 45, 'c': 46, 'd': 47, 'e': 48, 'f': 49, 'g': 50, 'h': 51, 'i': 52, 'j': 53, 'k': 54, 'l': 55, 'm': 56, 'n': 57, 'o': 58, 'p': 59, 'q': 60, 'r': 61, 's': 62, 't': 63, 'u': 64, 'v': 65, 'w': 66, 'x': 67, 'y': 68, 'z': 69}\n",
      "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '\"': 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '5': 10, ':': 11, '?': 12, 'I': 13, '|': 14, 'ँ': 15, 'ं': 16, 'ः': 17, 'अ': 18, 'आ': 19, 'इ': 20, 'ई': 21, 'उ': 22, 'ऊ': 23, 'ऋ': 24, 'ए': 25, 'ऐ': 26, 'ऑ': 27, 'ओ': 28, 'औ': 29, 'क': 30, 'ख': 31, 'ग': 32, 'घ': 33, 'च': 34, 'छ': 35, 'ज': 36, 'झ': 37, 'ञ': 38, 'ट': 39, 'ठ': 40, 'ड': 41, 'ढ': 42, 'ण': 43, 'त': 44, 'थ': 45, 'द': 46, 'ध': 47, 'न': 48, 'प': 49, 'फ': 50, 'ब': 51, 'भ': 52, 'म': 53, 'य': 54, 'र': 55, 'ल': 56, 'व': 57, 'श': 58, 'ष': 59, 'स': 60, 'ह': 61, '़': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॅ': 69, 'े': 70, 'ै': 71, 'ॉ': 72, 'ो': 73, 'ौ': 74, '्': 75, '।': 76, '०': 77, '१': 78, '२': 79, '४': 80, '५': 81, '६': 82, '७': 83, '८': 84, '९': 85, '\\u200d': 86}\n"
     ]
    }
   ],
   "source": [
    "input_token_index = dict([(char,i) for i,char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char,i) for i,char in enumerate(target_characters)])\n",
    "print(input_token_index)\n",
    "print(target_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_len, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_len, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_len, num_decoder_tokens), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t+1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i,t,target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i,t-1,target_token_index[char]] = 1.\n",
    "    decoder_input_data[i,t+1:,target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i,t:,target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 70)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it\n",
    "encoder_inputs = Input(shape=(None,num_encoder_tokens))\n",
    "encoder=LSTM(latent_dim,return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states\n",
    "encoder_states=[state_h,state_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up decoder , using ` encoder_states` as initial state\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well . we don't use the \n",
    "# return states in the training model , but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense= Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs= decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 29s 691ms/step - loss: 1.7165 - accuracy: 0.7140 - val_loss: 1.5895 - val_accuracy: 0.6897\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 23s 658ms/step - loss: 0.8669 - accuracy: 0.8065 - val_loss: 1.5141 - val_accuracy: 0.6912\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 23s 657ms/step - loss: 0.8318 - accuracy: 0.8100 - val_loss: 1.4394 - val_accuracy: 0.6912\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 23s 657ms/step - loss: 0.7905 - accuracy: 0.8121 - val_loss: 1.7417 - val_accuracy: 0.6927\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 23s 661ms/step - loss: 0.7513 - accuracy: 0.8149 - val_loss: 1.2340 - val_accuracy: 0.7014\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.7081 - accuracy: 0.8260 - val_loss: 1.1587 - val_accuracy: 0.7132\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 23s 657ms/step - loss: 0.6526 - accuracy: 0.8366 - val_loss: 1.0843 - val_accuracy: 0.7231\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 23s 656ms/step - loss: 0.6081 - accuracy: 0.8451 - val_loss: 1.0516 - val_accuracy: 0.7315\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.5792 - accuracy: 0.8504 - val_loss: 1.0235 - val_accuracy: 0.7330\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 23s 653ms/step - loss: 0.5569 - accuracy: 0.8551 - val_loss: 0.9601 - val_accuracy: 0.7466\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 23s 655ms/step - loss: 0.5317 - accuracy: 0.8606 - val_loss: 0.9294 - val_accuracy: 0.7533\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.5176 - accuracy: 0.8633 - val_loss: 0.9171 - val_accuracy: 0.7582\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 23s 653ms/step - loss: 0.5098 - accuracy: 0.8653 - val_loss: 0.8766 - val_accuracy: 0.7658\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 23s 655ms/step - loss: 0.4893 - accuracy: 0.8698 - val_loss: 0.8648 - val_accuracy: 0.7712\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.4796 - accuracy: 0.8724 - val_loss: 0.8765 - val_accuracy: 0.7675\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 23s 651ms/step - loss: 0.4714 - accuracy: 0.8738 - val_loss: 0.8586 - val_accuracy: 0.7708\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 23s 651ms/step - loss: 0.4672 - accuracy: 0.8744 - val_loss: 0.8367 - val_accuracy: 0.7753\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 23s 653ms/step - loss: 0.4572 - accuracy: 0.8761 - val_loss: 0.8260 - val_accuracy: 0.7769\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 23s 653ms/step - loss: 0.4543 - accuracy: 0.8758 - val_loss: 0.8219 - val_accuracy: 0.7779\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.4436 - accuracy: 0.8784 - val_loss: 0.8278 - val_accuracy: 0.7757\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 23s 654ms/step - loss: 0.4332 - accuracy: 0.8809 - val_loss: 0.8152 - val_accuracy: 0.7789\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 23s 653ms/step - loss: 0.4304 - accuracy: 0.8807 - val_loss: 0.8012 - val_accuracy: 0.7811\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 23s 651ms/step - loss: 0.4297 - accuracy: 0.8806 - val_loss: 0.7936 - val_accuracy: 0.7829\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 23s 655ms/step - loss: 0.4201 - accuracy: 0.8825 - val_loss: 0.7892 - val_accuracy: 0.7842\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 23s 651ms/step - loss: 0.4170 - accuracy: 0.8830 - val_loss: 0.7878 - val_accuracy: 0.7848\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.4131 - accuracy: 0.8839 - val_loss: 0.7779 - val_accuracy: 0.7867\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 0.4065 - accuracy: 0.8864 - val_loss: 0.7818 - val_accuracy: 0.7861\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 23s 651ms/step - loss: 0.4029 - accuracy: 0.8867 - val_loss: 0.7718 - val_accuracy: 0.7881\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 23s 651ms/step - loss: 0.3958 - accuracy: 0.8885 - val_loss: 0.7722 - val_accuracy: 0.7889\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.3946 - accuracy: 0.8884 - val_loss: 0.7720 - val_accuracy: 0.7886\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 23s 653ms/step - loss: 0.3904 - accuracy: 0.8900 - val_loss: 0.7636 - val_accuracy: 0.7897\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.3839 - accuracy: 0.8920 - val_loss: 0.7597 - val_accuracy: 0.7917\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.3794 - accuracy: 0.8935 - val_loss: 0.7712 - val_accuracy: 0.7898\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.3830 - accuracy: 0.8921 - val_loss: 0.7643 - val_accuracy: 0.7901\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.3705 - accuracy: 0.8958 - val_loss: 0.7688 - val_accuracy: 0.7921\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.3686 - accuracy: 0.8959 - val_loss: 0.7608 - val_accuracy: 0.7902\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.3773 - accuracy: 0.8941 - val_loss: 0.7595 - val_accuracy: 0.7924\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.3630 - accuracy: 0.8980 - val_loss: 0.7511 - val_accuracy: 0.7944\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.3615 - accuracy: 0.8975 - val_loss: 0.7690 - val_accuracy: 0.7903\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.3546 - accuracy: 0.8997 - val_loss: 0.7523 - val_accuracy: 0.7948\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.3487 - accuracy: 0.9015 - val_loss: 0.7689 - val_accuracy: 0.7919\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.3451 - accuracy: 0.9021 - val_loss: 0.7541 - val_accuracy: 0.7952\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.3368 - accuracy: 0.9042 - val_loss: 0.7582 - val_accuracy: 0.7932\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.3387 - accuracy: 0.9046 - val_loss: 0.7564 - val_accuracy: 0.7969\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 23s 651ms/step - loss: 0.3337 - accuracy: 0.9057 - val_loss: 0.7565 - val_accuracy: 0.7954\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.3256 - accuracy: 0.9082 - val_loss: 0.7612 - val_accuracy: 0.7948\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.3242 - accuracy: 0.9084 - val_loss: 0.7522 - val_accuracy: 0.7975\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.3151 - accuracy: 0.9111 - val_loss: 0.7771 - val_accuracy: 0.7905\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.3150 - accuracy: 0.9111 - val_loss: 0.7612 - val_accuracy: 0.7965\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.3090 - accuracy: 0.9131 - val_loss: 0.7688 - val_accuracy: 0.7943\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.3035 - accuracy: 0.9141 - val_loss: 0.7751 - val_accuracy: 0.7920\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.2998 - accuracy: 0.9153 - val_loss: 0.7727 - val_accuracy: 0.7944\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.2963 - accuracy: 0.9164 - val_loss: 0.7591 - val_accuracy: 0.7962\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2912 - accuracy: 0.9186 - val_loss: 0.7801 - val_accuracy: 0.7945\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.2879 - accuracy: 0.9188 - val_loss: 0.7713 - val_accuracy: 0.7964\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.2854 - accuracy: 0.9197 - val_loss: 0.7877 - val_accuracy: 0.7942\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.2822 - accuracy: 0.9208 - val_loss: 0.8028 - val_accuracy: 0.7931\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 23s 651ms/step - loss: 0.2767 - accuracy: 0.9222 - val_loss: 0.8055 - val_accuracy: 0.7919\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 23s 646ms/step - loss: 0.2737 - accuracy: 0.9231 - val_loss: 0.8008 - val_accuracy: 0.7934\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2688 - accuracy: 0.9248 - val_loss: 0.8042 - val_accuracy: 0.7920\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.2663 - accuracy: 0.9254 - val_loss: 0.8098 - val_accuracy: 0.7910\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2655 - accuracy: 0.9253 - val_loss: 0.8104 - val_accuracy: 0.7931\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2581 - accuracy: 0.9274 - val_loss: 0.8124 - val_accuracy: 0.7914\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2569 - accuracy: 0.9280 - val_loss: 0.8135 - val_accuracy: 0.7916\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2495 - accuracy: 0.9302 - val_loss: 0.8217 - val_accuracy: 0.7930\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.2459 - accuracy: 0.9315 - val_loss: 0.8315 - val_accuracy: 0.7916\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.2418 - accuracy: 0.9325 - val_loss: 0.8332 - val_accuracy: 0.7926\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.2375 - accuracy: 0.9333 - val_loss: 0.8438 - val_accuracy: 0.7901\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 23s 646ms/step - loss: 0.2338 - accuracy: 0.9351 - val_loss: 0.8411 - val_accuracy: 0.7919\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 23s 651ms/step - loss: 0.2310 - accuracy: 0.9359 - val_loss: 0.8526 - val_accuracy: 0.7899\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2274 - accuracy: 0.9368 - val_loss: 0.8678 - val_accuracy: 0.7890\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.2233 - accuracy: 0.9379 - val_loss: 0.8645 - val_accuracy: 0.7896\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 23s 646ms/step - loss: 0.2190 - accuracy: 0.9392 - val_loss: 0.8759 - val_accuracy: 0.7874\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 23s 646ms/step - loss: 0.2182 - accuracy: 0.9398 - val_loss: 0.8819 - val_accuracy: 0.7880\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2118 - accuracy: 0.9416 - val_loss: 0.8869 - val_accuracy: 0.7867\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.2098 - accuracy: 0.9423 - val_loss: 0.8974 - val_accuracy: 0.7869\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.2066 - accuracy: 0.9428 - val_loss: 0.9001 - val_accuracy: 0.7878\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2078 - accuracy: 0.9425 - val_loss: 0.9116 - val_accuracy: 0.7855\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.2006 - accuracy: 0.9445 - val_loss: 0.9111 - val_accuracy: 0.7874\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1970 - accuracy: 0.9458 - val_loss: 0.9299 - val_accuracy: 0.7857\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1946 - accuracy: 0.9466 - val_loss: 0.9280 - val_accuracy: 0.7850\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.1913 - accuracy: 0.9478 - val_loss: 0.9411 - val_accuracy: 0.7847\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.1864 - accuracy: 0.9492 - val_loss: 0.9487 - val_accuracy: 0.7844\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.1869 - accuracy: 0.9487 - val_loss: 0.9424 - val_accuracy: 0.7860\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1846 - accuracy: 0.9494 - val_loss: 0.9466 - val_accuracy: 0.7862\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1809 - accuracy: 0.9505 - val_loss: 0.9594 - val_accuracy: 0.7845\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.1780 - accuracy: 0.9507 - val_loss: 0.9592 - val_accuracy: 0.7850\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 23s 650ms/step - loss: 0.1744 - accuracy: 0.9519 - val_loss: 0.9632 - val_accuracy: 0.7840\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1711 - accuracy: 0.9530 - val_loss: 0.9840 - val_accuracy: 0.7847\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.1695 - accuracy: 0.9541 - val_loss: 0.9800 - val_accuracy: 0.7844\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 23s 653ms/step - loss: 0.1665 - accuracy: 0.9544 - val_loss: 1.0082 - val_accuracy: 0.7819\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.1626 - accuracy: 0.9558 - val_loss: 1.0051 - val_accuracy: 0.7826\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1607 - accuracy: 0.9559 - val_loss: 1.0353 - val_accuracy: 0.7809\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 23s 647ms/step - loss: 0.1593 - accuracy: 0.9566 - val_loss: 1.0163 - val_accuracy: 0.7829\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1547 - accuracy: 0.9581 - val_loss: 1.0308 - val_accuracy: 0.7829\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1561 - accuracy: 0.9578 - val_loss: 1.0300 - val_accuracy: 0.7825\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1527 - accuracy: 0.9586 - val_loss: 1.0352 - val_accuracy: 0.7834\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 23s 649ms/step - loss: 0.1511 - accuracy: 0.9591 - val_loss: 1.0552 - val_accuracy: 0.7820\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 23s 648ms/step - loss: 0.1477 - accuracy: 0.9598 - val_loss: 1.0488 - val_accuracy: 0.7818\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 23s 646ms/step - loss: 0.1449 - accuracy: 0.9611 - val_loss: 1.0618 - val_accuracy: 0.7813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb2c8fb1c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn \n",
    "# `encoder_input_data` and `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size= batch_size, epochs = epochs , \n",
    "           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference setup\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_inputs_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_inputs_c = Input(shape=(latent_dim, ))\n",
    "decoder_states_inputs = [decoder_state_inputs_h, decoder_state_inputs_c]\n",
    "decoder_outputs , state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs]+decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference loop\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors\n",
    "    states_value=encoder_model.predict(input_seq)\n",
    "    \n",
    "    #Generate empty target sequence length 1 \n",
    "    target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "    # Populate the 1st character of target sequence with start character \n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "    \n",
    "    #Sampling loop for a batch of sequences \n",
    "    # (to samplify, here we assume a batch of size 1 )\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        #sample a token\n",
    "        sampled_token_index= np.argmax(output_tokens[0,-1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        #Exit condition : either hit max length\n",
    "        #or find stop character.\n",
    "        if(sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "        \n",
    "        #Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        #Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Help!\n",
      "उसकी बार की ज़ियलग गा।\n",
      "\n",
      "-\n",
      "Jump.\n",
      "मुझे कल आए ही कि मेट ग़लन मालूूँगा।\n",
      "\n",
      "-\n",
      "Jump.\n",
      "मुझे कल आए ही कि मेट ग़लन मालूूँगा।\n",
      "\n",
      "-\n",
      "Jump.\n",
      "मुझे कल आए ही कि मेट ग़लन मालूूँगा।\n",
      "\n",
      "-\n",
      "Hello!\n",
      "उसकी बार की ज़ियलग गा।\n",
      "\n",
      "-\n",
      "Hello!\n",
      "उसकी बार की ज़ियलग गा।\n",
      "\n",
      "-\n",
      "Cheers!\n",
      "मुझे कल आए ही कि मेट ग़लन माले जा अक्छा लग ला।\n",
      "\n",
      "-\n",
      "Cheers!\n",
      "मुझे कल आए ही कि मेट ग़लन माले जा अक्छा लग ला।\n",
      "\n",
      "-\n",
      "Got it?\n",
      "मुझे कल आए ही कि मही आँगा।\n",
      "\n",
      "-\n",
      "I'm OK.\n",
      "मैं तुम्हें यह बहुत नहीं हूँ।\n",
      "\n",
      "-\n",
      "Awesome!\n",
      "मुझे कल आए ही कि मेट ग़लन माले जा अक्छा लग ला।\n",
      "\n",
      "-\n",
      "Come in.\n",
      "मुझे कल बात किया हो हैं।\n",
      "\n",
      "-\n",
      "Get out!\n",
      "मुझे कल आए ही कि मही आँगा।\n",
      "\n",
      "-\n",
      "Go away!\n",
      "मुझे कल आए ही कि मही आँगा।\n",
      "\n",
      "-\n",
      "Goodbye!\n",
      "मुझे कल आए ही कि मेट ग़लन माले जा अक्छा को लिए बहुत करता है।\n",
      "\n",
      "-\n",
      "Perfect!\n",
      "मुझे उसके प्रस्ताव को अच्छा कर लड़ा था।\n",
      "\n",
      "-\n",
      "Perfect!\n",
      "मुझे उसके प्रस्ताव को अच्छा कर लड़ा था।\n",
      "\n",
      "-\n",
      "Welcome.\n",
      "तुम कितने साल के हो?\n",
      "\n",
      "-\n",
      "Welcome.\n",
      "तुम कितने साल के हो?\n",
      "\n",
      "-\n",
      "Have fun.\n",
      "मेरे पापा अभी अभी घर वापस आएँ हैं।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(20):\n",
    "    \n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print(input_texts[seq_index])\n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd = pd.read_csv('English_hindi.csv')\n",
    "cd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
