{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResultPN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhSQB5kTNXIj",
        "outputId": "8018b2ac-0bdd-4028-a4cc-e061a50adb6e"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "import os\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_Qu8xtrN2ZY",
        "outputId": "b62e652e-07db-4284-ce59-4b0c459669ac"
      },
      "source": [
        "# lines=pd.read_csv(\"/content/drive/MyDrive/Hindi_English_Truncated_Corpus.csv\",encoding='utf-8')\n",
        "url = 'https://raw.githubusercontent.com/prismspeechproject/neural/master/implementation/English_hindi.csv'\n",
        "lines=pd.read_csv(url, encoding='utf-8')\n",
        "lines= lines[lines.columns[0:2]]\n",
        "lines=lines[~pd.isnull(lines['English'])]\n",
        "lines=lines[~pd.isnull(lines['Hindi'])]\n",
        "lines.drop_duplicates(inplace=True)\n",
        "lines=lines.sample(n=25000,random_state=42)\n",
        "lines.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7,8,9,10,11,12,13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWVKuVwMOd4L"
      },
      "source": [
        "lines['English']=lines['English'].apply(lambda x: x.lower())\n",
        "lines['Hindi']=lines['Hindi'].apply(lambda x: x.lower())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n__eLPt5OikU"
      },
      "source": [
        "\n",
        "lines['English']=lines['English'].apply(lambda x: re.sub(\"'\", '', x))\n",
        "lines['Hindi']=lines['Hindi'].apply(lambda x: re.sub(\"'\", '', x))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axiD7EUOOlCj"
      },
      "source": [
        "exclude = set(string.punctuation) \n",
        "lines['English']=lines['English'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "lines['Hindi']=lines['Hindi'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcZG0cSsOnXO"
      },
      "source": [
        "remove_digits = str.maketrans('', '', digits)\n",
        "lines['English']=lines['English'].apply(lambda x: x.translate(remove_digits))\n",
        "lines['Hindi']=lines['Hindi'].apply(lambda x: x.translate(remove_digits))\n",
        "\n",
        "lines['Hindi'] = lines['Hindi'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "\n",
        "lines['English']=lines['English'].apply(lambda x: x.strip())\n",
        "lines['Hindi']=lines['Hindi'].apply(lambda x: x.strip())\n",
        "lines['English']=lines['English'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "lines['Hindi']=lines['Hindi'].apply(lambda x: re.sub(\" +\", \" \", x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "QiIRU5KJOqhL",
        "outputId": "1115e30b-6948-43c3-b4c5-6c9ca6ae0b0f"
      },
      "source": [
        "\n",
        "lines['Hindi'] = lines['Hindi'].apply(lambda x : 'START_ '+ x + ' _END')\n",
        "lines.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Hindi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>93994</th>\n",
              "      <td>it is the land of festivals of different religions like diwali dushehra pongal and onam idulfitr muhrram chrimas ester are very much famous</td>\n",
              "      <td>START_ विभिन्न धर्मों के इस भूभाग पर कई मनभावन पर्व त्यौहार मनाए जाते हैं दिवाली होली दशहरा पोंगल तथा ओणम ईदउलफितर मुहर्रम क्रिसमस ईस्टर आदि भी काफ़ी लोकप्रिय हैं। _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124301</th>\n",
              "      <td>enceladus seen here</td>\n",
              "      <td>START_ एनसेलेडस पर इस जगह _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28869</th>\n",
              "      <td>ghanaian businessmen south african enterprising leaders</td>\n",
              "      <td>START_ या फिर घाना के व्यवसायियों से या दक्षिण अफ्रिका के उद्योगों में अग्रणी भूमिका निभाने वालों से _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54938</th>\n",
              "      <td>the burma oil company processed the crude in its refinery at rangoon and distributed the products mainly kerosene through its tank installations at chief indian ports and through a fleet of steam tankers</td>\n",
              "      <td>START_ बर्मा आयल कंपनी अपनी रंगून की रिफाइनरी में कच्चे तेल को साफ करती थी और अपने उत्पादन का वितरण जो मुख़्यत मिट्टी का तेल था प्रधान भारतीय बंदरगाहों पर लगे टैंक प्रतिष्ठानों के द्वारा तथा स्टीम टैंकरों के द्वारा करती थी _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>ismail afandi aka ismail khan of the ottoman empire designer of the main dome</td>\n",
              "      <td>START_ मुख्य गुम्बद का अभिकल्पक इस्माइल एकाइस्माइल खाँ जो कि ऑट्टोमन साम्राज्य का प्रमुख गोलार्ध एवं गुम्बद अभिकल्पक थे। _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                            English                                                                                                                                                                                                                                Hindi\n",
              "93994   it is the land of festivals of different religions like diwali dushehra pongal and onam idulfitr muhrram chrimas ester are very much famous                                                                  START_ विभिन्न धर्मों के इस भूभाग पर कई मनभावन पर्व त्यौहार मनाए जाते हैं दिवाली होली दशहरा पोंगल तथा ओणम ईदउलफितर मुहर्रम क्रिसमस ईस्टर आदि भी काफ़ी लोकप्रिय हैं। _END                                                           \n",
              "124301  enceladus seen here                                                                                                                                                                                          START_ एनसेलेडस पर इस जगह _END                                                                                                                                                                                                     \n",
              "28869   ghanaian businessmen south african enterprising leaders                                                                                                                                                      START_ या फिर घाना के व्यवसायियों से या दक्षिण अफ्रिका के उद्योगों में अग्रणी भूमिका निभाने वालों से _END                                                                                                                          \n",
              "54938   the burma oil company processed the crude in its refinery at rangoon and distributed the products mainly kerosene through its tank installations at chief indian ports and through a fleet of steam tankers  START_ बर्मा आयल कंपनी अपनी रंगून की रिफाइनरी में कच्चे तेल को साफ करती थी और अपने उत्पादन का वितरण जो मुख़्यत मिट्टी का तेल था प्रधान भारतीय बंदरगाहों पर लगे टैंक प्रतिष्ठानों के द्वारा तथा स्टीम टैंकरों के द्वारा करती थी _END\n",
              "49997   ismail afandi aka ismail khan of the ottoman empire designer of the main dome                                                                                                                                START_ मुख्य गुम्बद का अभिकल्पक इस्माइल एकाइस्माइल खाँ जो कि ऑट्टोमन साम्राज्य का प्रमुख गोलार्ध एवं गुम्बद अभिकल्पक थे। _END                                                                                                      "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UA5CKo2OuQy"
      },
      "source": [
        "\n",
        "all_eng_words=set()\n",
        "for eng in lines['English']:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)\n",
        "\n",
        "all_hindi_words=set()\n",
        "for hin in lines['Hindi']:\n",
        "    for word in hin.split():\n",
        "        if word not in all_hindi_words:\n",
        "            all_hindi_words.add(word)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJr_V-G2OyD7",
        "outputId": "1b72d8a5-bffb-4070-e417-92a02541f5c1"
      },
      "source": [
        "len(all_eng_words)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30829"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrs_Q7ieOy7e",
        "outputId": "49ae962a-70d9-4dc3-9bf3-071487756926"
      },
      "source": [
        "len(all_hindi_words)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37434"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvII-gQ-PAYJ"
      },
      "source": [
        "lines['length_eng_sentence']=lines['English'].apply(lambda x:len(x.split(\" \")))\n",
        "lines['length_hin_sentence']=lines['Hindi'].apply(lambda x:len(x.split(\" \")))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "dE8rgGM0PEdh",
        "outputId": "e9b284c3-b86b-40a0-8d60-cf297de65935"
      },
      "source": [
        "lines.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Hindi</th>\n",
              "      <th>length_eng_sentence</th>\n",
              "      <th>length_hin_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>93994</th>\n",
              "      <td>it is the land of festivals of different religions like diwali dushehra pongal and onam idulfitr muhrram chrimas ester are very much famous</td>\n",
              "      <td>START_ विभिन्न धर्मों के इस भूभाग पर कई मनभावन पर्व त्यौहार मनाए जाते हैं दिवाली होली दशहरा पोंगल तथा ओणम ईदउलफितर मुहर्रम क्रिसमस ईस्टर आदि भी काफ़ी लोकप्रिय हैं। _END</td>\n",
              "      <td>23</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124301</th>\n",
              "      <td>enceladus seen here</td>\n",
              "      <td>START_ एनसेलेडस पर इस जगह _END</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28869</th>\n",
              "      <td>ghanaian businessmen south african enterprising leaders</td>\n",
              "      <td>START_ या फिर घाना के व्यवसायियों से या दक्षिण अफ्रिका के उद्योगों में अग्रणी भूमिका निभाने वालों से _END</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54938</th>\n",
              "      <td>the burma oil company processed the crude in its refinery at rangoon and distributed the products mainly kerosene through its tank installations at chief indian ports and through a fleet of steam tankers</td>\n",
              "      <td>START_ बर्मा आयल कंपनी अपनी रंगून की रिफाइनरी में कच्चे तेल को साफ करती थी और अपने उत्पादन का वितरण जो मुख़्यत मिट्टी का तेल था प्रधान भारतीय बंदरगाहों पर लगे टैंक प्रतिष्ठानों के द्वारा तथा स्टीम टैंकरों के द्वारा करती थी _END</td>\n",
              "      <td>33</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>ismail afandi aka ismail khan of the ottoman empire designer of the main dome</td>\n",
              "      <td>START_ मुख्य गुम्बद का अभिकल्पक इस्माइल एकाइस्माइल खाँ जो कि ऑट्टोमन साम्राज्य का प्रमुख गोलार्ध एवं गुम्बद अभिकल्पक थे। _END</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                            English                                                                                                                                                                                                                                Hindi  length_eng_sentence  length_hin_sentence\n",
              "93994   it is the land of festivals of different religions like diwali dushehra pongal and onam idulfitr muhrram chrimas ester are very much famous                                                                  START_ विभिन्न धर्मों के इस भूभाग पर कई मनभावन पर्व त्यौहार मनाए जाते हैं दिवाली होली दशहरा पोंगल तथा ओणम ईदउलफितर मुहर्रम क्रिसमस ईस्टर आदि भी काफ़ी लोकप्रिय हैं। _END                                                             23                   30                 \n",
              "124301  enceladus seen here                                                                                                                                                                                          START_ एनसेलेडस पर इस जगह _END                                                                                                                                                                                                       3                    6                  \n",
              "28869   ghanaian businessmen south african enterprising leaders                                                                                                                                                      START_ या फिर घाना के व्यवसायियों से या दक्षिण अफ्रिका के उद्योगों में अग्रणी भूमिका निभाने वालों से _END                                                                                                                            6                    19                 \n",
              "54938   the burma oil company processed the crude in its refinery at rangoon and distributed the products mainly kerosene through its tank installations at chief indian ports and through a fleet of steam tankers  START_ बर्मा आयल कंपनी अपनी रंगून की रिफाइनरी में कच्चे तेल को साफ करती थी और अपने उत्पादन का वितरण जो मुख़्यत मिट्टी का तेल था प्रधान भारतीय बंदरगाहों पर लगे टैंक प्रतिष्ठानों के द्वारा तथा स्टीम टैंकरों के द्वारा करती थी _END  33                   43                 \n",
              "49997   ismail afandi aka ismail khan of the ottoman empire designer of the main dome                                                                                                                                START_ मुख्य गुम्बद का अभिकल्पक इस्माइल एकाइस्माइल खाँ जो कि ऑट्टोमन साम्राज्य का प्रमुख गोलार्ध एवं गुम्बद अभिकल्पक थे। _END                                                                                                        14                   20                 "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UYfBtnSPHOB"
      },
      "source": [
        "lines=lines[lines['length_eng_sentence']<=20]\n",
        "lines=lines[lines['length_hin_sentence']<=20]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Yic0-btPJ6n",
        "outputId": "ccd427fd-d111-4844-c721-af3357e82bc0"
      },
      "source": [
        "print(\"maximum length of Hindi Sentence \",max(lines['length_hin_sentence']))\n",
        "print(\"maximum length of English Sentence \",max(lines['length_eng_sentence']))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum length of Hindi Sentence  20\n",
            "maximum length of English Sentence  20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlwgKcUAPM1E"
      },
      "source": [
        "max_length_src=max(lines['length_hin_sentence'])\n",
        "max_length_tar=max(lines['length_eng_sentence'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHrXSf05PTT_",
        "outputId": "3eb47299-4a1b-469f-9fa1-942d21a30d94"
      },
      "source": [
        "input_words = sorted(list(all_eng_words))\n",
        "target_words = sorted(list(all_hindi_words))\n",
        "num_encoder_tokens = len(all_eng_words)\n",
        "num_decoder_tokens = len(all_hindi_words)\n",
        "num_encoder_tokens, num_decoder_tokens"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30829, 37434)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGIPlxUsPVdN"
      },
      "source": [
        "num_decoder_tokens += 1 \n",
        "num_encoder_tokens = num_encoder_tokens + 1 "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZLTfrRLPYJz"
      },
      "source": [
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myDq4CvQPaja"
      },
      "source": [
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "Y45WqrdqPct7",
        "outputId": "60f04d32-e1f8-4a8d-a260-326720412740"
      },
      "source": [
        "lines = shuffle(lines)\n",
        "lines.head(10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Hindi</th>\n",
              "      <th>length_eng_sentence</th>\n",
              "      <th>length_hin_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20628</th>\n",
              "      <td>he received nobel prize for literature in for his poetical composition ‘geetanjali</td>\n",
              "      <td>START_ उनकी काव्यरचना गीतांजलि के लिये उन्हे सन में साहित्य का नोबेल पुरस्कार मिला। _END</td>\n",
              "      <td>12</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46802</th>\n",
              "      <td>reduce the incidence of invasive cervical cancer by approximately</td>\n",
              "      <td>START_ इनवेसिव सरवाइकल ह्ययोनि ग्रीवा संबंधी कैंसरहृ की घटनाओं में लगभग प्रतिशत तक कमी _END</td>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114466</th>\n",
              "      <td>rajasthan is bharats very important region</td>\n",
              "      <td>START_ राजस्थान भारत का एक महत्ती प्रांत है। _END</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6844</th>\n",
              "      <td>the leg muscles may get cramps weakness or discomfort on walking</td>\n",
              "      <td>START_ इसके कारण पैरों की मांसपेशियों में तेज दर्द या कमजोरी अथवा चलने पर तकलीफ होने लगती है _END</td>\n",
              "      <td>11</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106869</th>\n",
              "      <td>in openness and inclusiveness</td>\n",
              "      <td>START_ एक अभ्यास शुरू किया _END</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30146</th>\n",
              "      <td>every shrine in tajmahal leads to a broad mehrabi basement</td>\n",
              "      <td>START_ ताजमहल में हरेक पुण्यस्थान एक वृहत मेहराबी तहखाने में खुलता है। _END</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80340</th>\n",
              "      <td>and its a little bit like a peace corps for geeks</td>\n",
              "      <td>START_ यूँ समझिये जैसे कंप्यूटर की दुनिया में ही रहनेवाले विचित्र विशेषज्ञ को शान्ति सेना में भेज दिया जाए _END</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104378</th>\n",
              "      <td>hussain a curious mix of bottle and whine in india said it over and over again</td>\n",
              "      <td>START_ भारत में खासे चिड़ेचिड़ै दिखे ह्सैन ने यह बात बारबार कही _END</td>\n",
              "      <td>16</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57390</th>\n",
              "      <td>there was jhasi ki rani regiment for ladies in azad hind army</td>\n",
              "      <td>START_ आज़ाद हिन्द फ़ौज में औरतो के लिए झाँसी की रानी रेजिमेंट भी बनायी गयी। _END</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84787</th>\n",
              "      <td>normality overlooks the beauty that differences give us</td>\n",
              "      <td>START_ साधारणता असाधारण सुंदरता को अनदेखा करती है _END</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                   English                                                                                                            Hindi  length_eng_sentence  length_hin_sentence\n",
              "20628   he received nobel prize for literature in for his poetical composition ‘geetanjali  START_ उनकी काव्यरचना गीतांजलि के लिये उन्हे सन में साहित्य का नोबेल पुरस्कार मिला। _END                         12                   15                 \n",
              "46802   reduce the incidence of invasive cervical cancer by approximately                   START_ इनवेसिव सरवाइकल ह्ययोनि ग्रीवा संबंधी कैंसरहृ की घटनाओं में लगभग प्रतिशत तक कमी _END                      9                    15                 \n",
              "114466  rajasthan is bharats very important region                                          START_ राजस्थान भारत का एक महत्ती प्रांत है। _END                                                                6                    9                  \n",
              "6844    the leg muscles may get cramps weakness or discomfort on walking                    START_ इसके कारण पैरों की मांसपेशियों में तेज दर्द या कमजोरी अथवा चलने पर तकलीफ होने लगती है _END                11                   19                 \n",
              "106869  in openness and inclusiveness                                                       START_ एक अभ्यास शुरू किया _END                                                                                  4                    6                  \n",
              "30146   every shrine in tajmahal leads to a broad mehrabi basement                          START_ ताजमहल में हरेक पुण्यस्थान एक वृहत मेहराबी तहखाने में खुलता है। _END                                      10                   13                 \n",
              "80340   and its a little bit like a peace corps for geeks                                   START_ यूँ समझिये जैसे कंप्यूटर की दुनिया में ही रहनेवाले विचित्र विशेषज्ञ को शान्ति सेना में भेज दिया जाए _END  11                   20                 \n",
              "104378  hussain a curious mix of bottle and whine in india said it over and over again      START_ भारत में खासे चिड़ेचिड़ै दिखे ह्सैन ने यह बात बारबार कही _END                                             16                   13                 \n",
              "57390   there was jhasi ki rani regiment for ladies in azad hind army                       START_ आज़ाद हिन्द फ़ौज में औरतो के लिए झाँसी की रानी रेजिमेंट भी बनायी गयी। _END                                12                   16                 \n",
              "84787   normality overlooks the beauty that differences give us                             START_ साधारणता असाधारण सुंदरता को अनदेखा करती है _END                                                           8                    9                  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQmUqNgwPfCD",
        "outputId": "aea2927c-30fe-4de4-cc8a-dc3efe941edf"
      },
      "source": [
        "X, y = lines['English'], lines['Hindi']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13168,), (3293,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XXetW1NPhmd"
      },
      "source": [
        "X_train.to_pickle('X_train.pkl')\n",
        "X_test.to_pickle('X_test.pkl')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzHWN4FYPkst"
      },
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
        "                    if t>0:\n",
        "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGcjLiK-PnHW"
      },
      "source": [
        "latent_dim=300\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDsciYUTPrZi"
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goTwXxdlPt1d",
        "outputId": "e13b88d9-c24b-41a6-ba87-c4074d7249e0"
      },
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "epochs = 100\n",
        "batch_size =128\n",
        "# checkpoint_path = \"/content/drive/MyDrive/Colab/ResultPNBy/\" /training_2/cp-{epoch:04d}.ckpt\"\n",
        "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     filepath=checkpoint_path, \n",
        "#     verbose=1, \n",
        "#     save_weights_only=True,\n",
        "#     save_freq=5*batch_size)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples/batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples/batch_size)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 300)    9249000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 300)    11230500    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 300), (None, 721200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 300),  721200      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 37435)  11267935    lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 33,189,835\n",
            "Trainable params: 33,189,835\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "102/102 [==============================] - 67s 407ms/step - loss: 4.0810 - val_loss: 3.4942\n",
            "Epoch 2/100\n",
            "102/102 [==============================] - 44s 426ms/step - loss: 3.4844 - val_loss: 3.4377\n",
            "Epoch 3/100\n",
            "102/102 [==============================] - 39s 381ms/step - loss: 3.3530 - val_loss: 3.3334\n",
            "Epoch 4/100\n",
            "102/102 [==============================] - 43s 424ms/step - loss: 3.2081 - val_loss: 3.2685\n",
            "Epoch 5/100\n",
            "102/102 [==============================] - 40s 385ms/step - loss: 3.0842 - val_loss: 3.2218\n",
            "Epoch 6/100\n",
            "102/102 [==============================] - 39s 382ms/step - loss: 2.9793 - val_loss: 3.2017\n",
            "Epoch 7/100\n",
            "102/102 [==============================] - 40s 394ms/step - loss: 2.8848 - val_loss: 3.1833\n",
            "Epoch 8/100\n",
            "102/102 [==============================] - 43s 421ms/step - loss: 2.7981 - val_loss: 3.1845\n",
            "Epoch 9/100\n",
            "102/102 [==============================] - 39s 383ms/step - loss: 2.7106 - val_loss: 3.1495\n",
            "Epoch 10/100\n",
            "102/102 [==============================] - 44s 426ms/step - loss: 2.6298 - val_loss: 3.1568\n",
            "Epoch 11/100\n",
            "102/102 [==============================] - 44s 427ms/step - loss: 2.5484 - val_loss: 3.1394\n",
            "Epoch 12/100\n",
            "102/102 [==============================] - 39s 381ms/step - loss: 2.4700 - val_loss: 3.1349\n",
            "Epoch 13/100\n",
            "102/102 [==============================] - 43s 419ms/step - loss: 2.3941 - val_loss: 3.1375\n",
            "Epoch 14/100\n",
            "102/102 [==============================] - 43s 418ms/step - loss: 2.3166 - val_loss: 3.1331\n",
            "Epoch 15/100\n",
            "102/102 [==============================] - 39s 381ms/step - loss: 2.2417 - val_loss: 3.1413\n",
            "Epoch 16/100\n",
            "102/102 [==============================] - 39s 377ms/step - loss: 2.1649 - val_loss: 3.1372\n",
            "Epoch 17/100\n",
            "102/102 [==============================] - 39s 381ms/step - loss: 2.0895 - val_loss: 3.1413\n",
            "Epoch 18/100\n",
            "102/102 [==============================] - 39s 385ms/step - loss: 2.0168 - val_loss: 3.1494\n",
            "Epoch 19/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 1.9393 - val_loss: 3.1570\n",
            "Epoch 20/100\n",
            "102/102 [==============================] - 40s 388ms/step - loss: 1.8694 - val_loss: 3.1659\n",
            "Epoch 21/100\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 1.7968 - val_loss: 3.1774\n",
            "Epoch 22/100\n",
            "102/102 [==============================] - 40s 385ms/step - loss: 1.7292 - val_loss: 3.1839\n",
            "Epoch 23/100\n",
            "102/102 [==============================] - 39s 380ms/step - loss: 1.6574 - val_loss: 3.1962\n",
            "Epoch 24/100\n",
            "102/102 [==============================] - 40s 387ms/step - loss: 1.5876 - val_loss: 3.2166\n",
            "Epoch 25/100\n",
            "102/102 [==============================] - 40s 386ms/step - loss: 1.5221 - val_loss: 3.2364\n",
            "Epoch 26/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 1.4572 - val_loss: 3.2456\n",
            "Epoch 27/100\n",
            "102/102 [==============================] - 39s 380ms/step - loss: 1.3899 - val_loss: 3.2682\n",
            "Epoch 28/100\n",
            "102/102 [==============================] - 39s 385ms/step - loss: 1.3283 - val_loss: 3.2862\n",
            "Epoch 29/100\n",
            "102/102 [==============================] - 39s 376ms/step - loss: 1.2646 - val_loss: 3.2983\n",
            "Epoch 30/100\n",
            "102/102 [==============================] - 39s 384ms/step - loss: 1.2017 - val_loss: 3.3221\n",
            "Epoch 31/100\n",
            "102/102 [==============================] - 43s 422ms/step - loss: 1.1416 - val_loss: 3.3338\n",
            "Epoch 32/100\n",
            "102/102 [==============================] - 40s 385ms/step - loss: 1.0867 - val_loss: 3.3612\n",
            "Epoch 33/100\n",
            "102/102 [==============================] - 39s 383ms/step - loss: 1.0289 - val_loss: 3.3754\n",
            "Epoch 34/100\n",
            "102/102 [==============================] - 39s 380ms/step - loss: 0.9767 - val_loss: 3.4019\n",
            "Epoch 35/100\n",
            "102/102 [==============================] - 39s 381ms/step - loss: 0.9249 - val_loss: 3.4187\n",
            "Epoch 36/100\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 0.8755 - val_loss: 3.4393\n",
            "Epoch 37/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.8232 - val_loss: 3.4654\n",
            "Epoch 38/100\n",
            "102/102 [==============================] - 43s 423ms/step - loss: 0.7782 - val_loss: 3.4904\n",
            "Epoch 39/100\n",
            "102/102 [==============================] - 39s 381ms/step - loss: 0.7310 - val_loss: 3.5050\n",
            "Epoch 40/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.6868 - val_loss: 3.5293\n",
            "Epoch 41/100\n",
            "102/102 [==============================] - 39s 376ms/step - loss: 0.6460 - val_loss: 3.5486\n",
            "Epoch 42/100\n",
            "102/102 [==============================] - 39s 375ms/step - loss: 0.6034 - val_loss: 3.5749\n",
            "Epoch 43/100\n",
            "102/102 [==============================] - 43s 421ms/step - loss: 0.5659 - val_loss: 3.5953\n",
            "Epoch 44/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.5282 - val_loss: 3.6158\n",
            "Epoch 45/100\n",
            "102/102 [==============================] - 39s 375ms/step - loss: 0.4928 - val_loss: 3.6360\n",
            "Epoch 46/100\n",
            "102/102 [==============================] - 38s 373ms/step - loss: 0.4579 - val_loss: 3.6602\n",
            "Epoch 47/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.4280 - val_loss: 3.6907\n",
            "Epoch 48/100\n",
            "102/102 [==============================] - 39s 382ms/step - loss: 0.3960 - val_loss: 3.7198\n",
            "Epoch 49/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.3644 - val_loss: 3.7345\n",
            "Epoch 50/100\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 0.3409 - val_loss: 3.7562\n",
            "Epoch 51/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.3144 - val_loss: 3.7845\n",
            "Epoch 52/100\n",
            "102/102 [==============================] - 39s 376ms/step - loss: 0.2903 - val_loss: 3.8020\n",
            "Epoch 53/100\n",
            "102/102 [==============================] - 39s 377ms/step - loss: 0.2683 - val_loss: 3.8310\n",
            "Epoch 54/100\n",
            "102/102 [==============================] - 39s 376ms/step - loss: 0.2460 - val_loss: 3.8584\n",
            "Epoch 55/100\n",
            "102/102 [==============================] - 38s 373ms/step - loss: 0.2256 - val_loss: 3.8686\n",
            "Epoch 56/100\n",
            "102/102 [==============================] - 39s 376ms/step - loss: 0.2068 - val_loss: 3.9054\n",
            "Epoch 57/100\n",
            "102/102 [==============================] - 43s 420ms/step - loss: 0.1901 - val_loss: 3.9138\n",
            "Epoch 58/100\n",
            "102/102 [==============================] - 39s 376ms/step - loss: 0.1733 - val_loss: 3.9429\n",
            "Epoch 59/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.1586 - val_loss: 3.9590\n",
            "Epoch 60/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.1465 - val_loss: 3.9908\n",
            "Epoch 61/100\n",
            "102/102 [==============================] - 38s 374ms/step - loss: 0.1328 - val_loss: 4.0178\n",
            "Epoch 62/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.1205 - val_loss: 4.0402\n",
            "Epoch 63/100\n",
            "102/102 [==============================] - 38s 374ms/step - loss: 0.1099 - val_loss: 4.0600\n",
            "Epoch 64/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.0979 - val_loss: 4.0810\n",
            "Epoch 65/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.0898 - val_loss: 4.0951\n",
            "Epoch 66/100\n",
            "102/102 [==============================] - 39s 381ms/step - loss: 0.0817 - val_loss: 4.1163\n",
            "Epoch 67/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.0742 - val_loss: 4.1297\n",
            "Epoch 68/100\n",
            "102/102 [==============================] - 43s 416ms/step - loss: 0.0666 - val_loss: 4.1539\n",
            "Epoch 69/100\n",
            "102/102 [==============================] - 39s 376ms/step - loss: 0.0604 - val_loss: 4.1715\n",
            "Epoch 70/100\n",
            "102/102 [==============================] - 38s 373ms/step - loss: 0.0563 - val_loss: 4.1895\n",
            "Epoch 71/100\n",
            "102/102 [==============================] - 38s 373ms/step - loss: 0.0517 - val_loss: 4.2119\n",
            "Epoch 72/100\n",
            "102/102 [==============================] - 39s 380ms/step - loss: 0.0469 - val_loss: 4.2162\n",
            "Epoch 73/100\n",
            "102/102 [==============================] - 38s 374ms/step - loss: 0.0421 - val_loss: 4.2407\n",
            "Epoch 74/100\n",
            "102/102 [==============================] - 39s 377ms/step - loss: 0.0399 - val_loss: 4.2504\n",
            "Epoch 75/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.0363 - val_loss: 4.2736\n",
            "Epoch 76/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.0322 - val_loss: 4.2880\n",
            "Epoch 77/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.0311 - val_loss: 4.3049\n",
            "Epoch 78/100\n",
            "102/102 [==============================] - 38s 373ms/step - loss: 0.0284 - val_loss: 4.3117\n",
            "Epoch 79/100\n",
            "102/102 [==============================] - 39s 380ms/step - loss: 0.0268 - val_loss: 4.3334\n",
            "Epoch 80/100\n",
            "102/102 [==============================] - 43s 418ms/step - loss: 0.0237 - val_loss: 4.3505\n",
            "Epoch 81/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.0229 - val_loss: 4.3602\n",
            "Epoch 82/100\n",
            "102/102 [==============================] - 38s 375ms/step - loss: 0.0212 - val_loss: 4.3684\n",
            "Epoch 83/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.0192 - val_loss: 4.3865\n",
            "Epoch 84/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.0188 - val_loss: 4.4012\n",
            "Epoch 85/100\n",
            "102/102 [==============================] - 43s 423ms/step - loss: 0.0172 - val_loss: 4.4156\n",
            "Epoch 86/100\n",
            "102/102 [==============================] - 43s 416ms/step - loss: 0.0162 - val_loss: 4.4316\n",
            "Epoch 87/100\n",
            "102/102 [==============================] - 38s 371ms/step - loss: 0.0149 - val_loss: 4.4451\n",
            "Epoch 88/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.0144 - val_loss: 4.4599\n",
            "Epoch 89/100\n",
            "102/102 [==============================] - 39s 376ms/step - loss: 0.0132 - val_loss: 4.4694\n",
            "Epoch 90/100\n",
            "102/102 [==============================] - 43s 415ms/step - loss: 0.0127 - val_loss: 4.4770\n",
            "Epoch 91/100\n",
            "102/102 [==============================] - 38s 374ms/step - loss: 0.0129 - val_loss: 4.4853\n",
            "Epoch 92/100\n",
            "102/102 [==============================] - 38s 374ms/step - loss: 0.0120 - val_loss: 4.5191\n",
            "Epoch 93/100\n",
            "102/102 [==============================] - 38s 373ms/step - loss: 0.0110 - val_loss: 4.5135\n",
            "Epoch 94/100\n",
            "102/102 [==============================] - 38s 372ms/step - loss: 0.0100 - val_loss: 4.5326\n",
            "Epoch 95/100\n",
            "102/102 [==============================] - 39s 381ms/step - loss: 0.0106 - val_loss: 4.5349\n",
            "Epoch 96/100\n",
            "102/102 [==============================] - 39s 379ms/step - loss: 0.0096 - val_loss: 4.5641\n",
            "Epoch 97/100\n",
            "102/102 [==============================] - 39s 376ms/step - loss: 0.0086 - val_loss: 4.5705\n",
            "Epoch 98/100\n",
            "102/102 [==============================] - 43s 421ms/step - loss: 0.0087 - val_loss: 4.5797\n",
            "Epoch 99/100\n",
            "102/102 [==============================] - 39s 377ms/step - loss: 0.0077 - val_loss: 4.5881\n",
            "Epoch 100/100\n",
            "102/102 [==============================] - 39s 378ms/step - loss: 0.0078 - val_loss: 4.5936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb2b9cbe990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqve1DcWsPS9",
        "outputId": "2e29fb06-73dd-4497-8a1b-d6545a9fe333"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu0WDaNWPwWU"
      },
      "source": [
        "import keras\n",
        "model.save('/content/drive/MyDrive/Colab/ResultPNA2.h5')\n",
        "# model = keras.models.load_model('/content/drive/MyDrive/ColabNotebooks/ResultPN.h5')\n",
        "new_model = keras.models.load_model('/content/drive/MyDrive/Colab/ResultPNA2.h5')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNmVd5nn8eKG"
      },
      "source": [
        "# new_model.get_weights()\n",
        "decoder_model.save_weights('/content/drive/MyDrive/Colab/ResultPNA2DecoderModelW.h5')\n",
        "# decoder_model.save('/content/drive/MyDrive/Colab/ResultPNA2DecoderModel.h5')\n",
        "encoder_model.save_weights('/content/drive/MyDrive/Colab/ResultPNA2EncoderModelW.h5')\n",
        "\n",
        "# import keras\n",
        "# encoder1_model = keras.loa('/content/drive/MyDrive/Colab/ResultPNA2EncoderModelW.h5')\n",
        "# decoder1_model = keras.Model.load_weights(new_model, filepath='/content/drive/MyDrive/Colab/ResultPNA2DecoderModelW.h5')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTpJAhxz_ymD",
        "outputId": "3c557b90-42ae-47b4-c09b-332203e58a7f"
      },
      "source": [
        "encoder_model.get_weights()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.03819279, -0.0322782 , -0.03939082, ...,  0.00416565,\n",
              "          0.02199462, -0.04040111],\n",
              "        [-0.24462663,  0.07550556, -0.02188983, ..., -0.05438171,\n",
              "         -0.05992344, -0.14644411],\n",
              "        [-0.02854334,  0.01091386,  0.06664548, ..., -0.00438381,\n",
              "          0.09545403, -0.08324859],\n",
              "        ...,\n",
              "        [-0.01087225,  0.05336149,  0.09070357, ..., -0.01562318,\n",
              "         -0.02250707, -0.18338294],\n",
              "        [-0.1053583 ,  0.04338017, -0.00409952, ...,  0.06310016,\n",
              "          0.02308963,  0.11768317],\n",
              "        [-0.00641337, -0.00235736, -0.00291197, ...,  0.06093942,\n",
              "         -0.07201329, -0.03548596]], dtype=float32),\n",
              " array([[-2.3507608e-01, -5.9964079e-02,  3.3216095e-01, ...,\n",
              "          1.4458965e-01,  2.2424659e-01, -2.0234863e-01],\n",
              "        [ 1.1874749e-01, -1.2757336e-01,  2.1041945e-02, ...,\n",
              "          2.0338273e-05, -2.1128204e-01,  2.9830477e-01],\n",
              "        [-7.7597684e-01, -2.7464086e-01, -8.0915622e-02, ...,\n",
              "         -2.5459874e-01, -1.7398483e-01, -1.4230114e-01],\n",
              "        ...,\n",
              "        [-3.8531911e-01, -1.6687453e-02, -1.6208571e-01, ...,\n",
              "          1.5333110e-01, -1.4224118e-02, -2.2187987e-02],\n",
              "        [ 5.7290369e-01,  2.6169011e-01,  2.5944138e-01, ...,\n",
              "          4.5420635e-03,  2.5646937e-01,  2.6176408e-01],\n",
              "        [-5.6411684e-01, -4.3227646e-02,  1.1736551e-02, ...,\n",
              "         -2.4091479e-01,  5.0451738e-01,  2.2263160e-01]], dtype=float32),\n",
              " array([[-0.5140193 ,  0.12388367, -0.08995879, ...,  0.24168658,\n",
              "         -0.07153021, -0.28896716],\n",
              "        [-0.00258562,  0.02860031, -0.03270608, ...,  0.07207741,\n",
              "          0.05939965,  0.26406434],\n",
              "        [ 0.02908781, -0.05320995, -0.06270514, ...,  0.1390821 ,\n",
              "         -0.00512369,  0.02164891],\n",
              "        ...,\n",
              "        [ 0.05001071,  0.06062172,  0.18688244, ..., -0.06610586,\n",
              "          0.10754993,  0.07867067],\n",
              "        [-0.06565181, -0.13549685, -0.1377466 , ...,  0.02419269,\n",
              "          0.07246044,  0.10446427],\n",
              "        [-0.18107332, -0.06860894,  0.15981804, ...,  0.01237164,\n",
              "          0.06070758,  0.01696055]], dtype=float32),\n",
              " array([ 0.3225542 ,  0.22125079,  0.35495254, ..., -0.0304928 ,\n",
              "         0.07943121,  0.5893062 ], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T61EJxUcRny7"
      },
      "source": [
        "# Encode the input sequence to get the \"thought vectors\"\n",
        "encoder1_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder1_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4YOw2fjFyfU"
      },
      "source": [
        "decoder1_model.load_weights('/content/drive/MyDrive/Colab/ResultPNA2DecoderModelW.h5')\n",
        "encoder1_model.load_weights('/content/drive/MyDrive/Colab/ResultPNA2EncoderModelW.h5')\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As1torELRsxv"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder1_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder1_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsFLiwePRvlh"
      },
      "source": [
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
        "k=-1"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZoQqfBPtKg6"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6foZmcBuRzd_",
        "outputId": "d568d2e6-ebf6-4e34-b395-4f227698b349"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: in the process he manages to strike a new romantic attitude\n",
            "Actual Hindi Translation:  इसी दौरान वह अपने में एक नया रोमानी बदलाव पाता है \n",
            "Predicted Hindi Translation:  इसी दौरान वह अपने में एक नया रोमानी बदलाव पाता है \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aid7gSvnCjke",
        "outputId": "6e21babf-0416-42e1-fc69-51365c7c6385"
      },
      "source": [
        "# type(decoded_sentence.split())\n",
        "y =y_train[k:k+1].values[0][6: -4].split()\n",
        "Y= decoded_sentence[:-4].split()\n",
        "# print(type(y), type(Y))\n",
        "print('BLEU-1: %f' % corpus_bleu(y, Y, weights=(1.0, 0, 0, 0)))\n",
        "print('BLEU-2: %f' % corpus_bleu(y, Y, weights=(0.5, 0.5, 0, 0)))\n",
        "print('BLEU-3: %f' % corpus_bleu(y, Y, weights=(0.33, 0.33, 0.33, 0)))\n",
        "print('BLEU-4: %f' % corpus_bleu(y, Y, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "# type(y_train.to_list())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.974359\n",
            "BLEU-2: 0.987096\n",
            "BLEU-3: 0.991465\n",
            "BLEU-4: 0.993527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJVl9nNs9L9U",
        "outputId": "885cdee3-9ec6-4666-faa9-9a5090cb9ef0"
      },
      "source": [
        "y, Y"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['इसी',\n",
              "   'दौरान',\n",
              "   'वह',\n",
              "   'अपने',\n",
              "   'में',\n",
              "   'एक',\n",
              "   'नया',\n",
              "   'रोमानी',\n",
              "   'बदलाव',\n",
              "   'पाता',\n",
              "   'है']],\n",
              " [['इसी',\n",
              "   'दौरान',\n",
              "   'वह',\n",
              "   'अपने',\n",
              "   'में',\n",
              "   'एक',\n",
              "   'नया',\n",
              "   'रोमानी',\n",
              "   'बदलाव',\n",
              "   'पाता',\n",
              "   'है']])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUksdbLwqoGP"
      },
      "source": [
        "help(corpus_bleu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vqo-6jSR2Dm"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhgnyZRcR41N"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FL2gMBWR5j_"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4dmLo7ttkLP"
      },
      "source": [
        "print('BLEU-1: %f' % corpus_bleu([y_train[k:k+1].values[0][6:-4].split()], [decoded_sentence[:-4].split()], weights=(1.0, 0, 0, 0)))\n",
        "print('BLEU-2: %f' % corpus_bleu(y_train[k:k+1].values[0][6:-4].split(), decoded_sentence[:-4].split(), weights=(0.5, 0.5, 0, 0)))\n",
        "print('BLEU-3: %f' % corpus_bleu(y_train[k:k+1].values[0][6:-4].split(), decoded_sentence[:-4].split(), weights=(0.33, 0.33, 0.33, 0)))\n",
        "print('BLEU-4: %f' % corpus_bleu(y_train[k:k+1].values[0][6:-4].split(), decoded_sentence[:-4].split(), weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-MmdBtptof9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fcf2ec-590f-4ad5-ac8e-04c6fd8152c3"
      },
      "source": [
        "print('Individual 1-gram: %f' % corpus_bleu(y, Y))\n",
        "print('Individual 2-gram: %f' % corpus_bleu(y, Y))\n",
        "print('Individual 3-gram: %f' % corpus_bleu(y, Y))\n",
        "print('Individual 4-gram: %f' % corpus_bleu(y, Y))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Individual 1-gram: 0.993527\n",
            "Individual 2-gram: 0.993527\n",
            "Individual 3-gram: 0.993527\n",
            "Individual 4-gram: 0.993527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soyj9VlVtui1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}