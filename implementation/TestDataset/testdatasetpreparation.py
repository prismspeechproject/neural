# -*- coding: utf-8 -*-
"""TestDatasetPreparation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10uA6CDahTHHF9Mz87iAiIRgi_VUlO_GE
"""

import pandas as pd
import string
import string
from sklearn.utils import shuffle
import unicodedata
import re
punctuations = set(string.punctuation)

digits = str.maketrans('', '', string.digits)
not_punc = ".?!,|"

for c in not_punc:
      punctuations.remove(c)

def create_dataset(url):
  df = pd.read_csv(url , encoding='utf-8')
  df = df[df.columns[0:2]]
  df = df[~pd.isnull(df['English'])]
  df.drop_duplicates(inplace=True)
  # Calculate number of words in each line of 2 columns
  df['len_eng_words'] = df['English'].apply(lambda e: len(str(e).split(" ")))
  df['len_hin_words'] = df['Hindi'].apply(lambda h: len(str(h).split(" ")))  
  return df

def unicode_to_ascii(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

def preprocessing_data_hin(w):
    w = unicode_to_ascii(str(w).lower().strip())
    w = re.sub(r"[\(\[].*?[\)\]]", ' ', w)
    w = re.sub(r"([.?!,])", r' \1 ', w)
    w = re.sub(r'["“”‘]', " ", w)
    w = re.sub(r'[0-9]', " ", w)
    
    w = w.strip()

    #w = '<start> ' + w + ' <end>'
    return w

def preprocessing_data_eng(w):
    w = unicode_to_ascii(str(w).lower().strip())
    w = re.sub(r"[\(\[].*?[\)\]]", '', w)
    w = re.sub(r"([.?!,])", r' \1 ', w)
    w = re.sub(r'["]', " ", w)
    w = re.sub(r'[^a-zA-Z?.!,]+', " ", w)

    w = w.strip()

    return w

def customize_dataset(df):
    df['Hindi'] = df['Hindi'].apply(lambda h : preprocessing_data_hin(h))
    df['English'] = df['English'].apply(lambda e : preprocessing_data_eng(e))
    df['Hindi'] = df['Hindi'].apply(lambda h: ''.join(c for c in h if c not in punctuations))
    df['English'] = df['English'].apply(lambda e: ''.join(c for c in e if c not in punctuations))
    df['English']=df['English'].apply(lambda e: e.translate(digits))
    df['Hindi']=df['Hindi'].apply(lambda h: h.translate(digits))
    df['Hindi']=df['Hindi'].apply(lambda h: re.sub("[a-z२३०८१५७९४६]", '', h))
    df['English'] = df['English'].apply(lambda e : '<start> ' + e + ' <end>')
    df['Hindi'] = df['Hindi'].apply(lambda h : '<start> ' + h + ' <end>')

    return df

def load_dataset():
    df = create_dataset('https://raw.githubusercontent.com/prismspeechproject/neural/master/implementation/English_hindi.csv')
    df = customize_dataset(df)

    return df

df = load_dataset()

len_words = int(input("Enter number words : "))
words = df[df['len_eng_words'] == len_words]
words.head()

words.to_csv('WordLen5.csv', index=None)

