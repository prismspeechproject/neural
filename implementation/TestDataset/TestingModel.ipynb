{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestingModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvB1PAv3RiRg"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding\n",
        "from keras.models import Model\n",
        "import preprocessdata as pe        ## It is builted module\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from keras.models import load_model\n",
        "# from sklearn.utils import shuffle\n",
        "# import unicodedata\n",
        "# import re\n",
        "# punctuations = set(string.punctuation)\n",
        "\n",
        "# digits_rm = str.maketrans('', '', string.digits)\n",
        "# not_punc = \".?!,|\"\n",
        "\n",
        "# for c in not_punc:\n",
        "#       punctuations.remove(c)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APHpu_r6SGGG"
      },
      "source": [
        "# def create_dataset(url, ):\n",
        "#   df = pd.read_csv(url , encoding='utf-8')\n",
        "#   df = df[df.columns[0:2]]\n",
        "#   df = df[~pd.isnull(df['English'])]\n",
        "#   df.drop_duplicates(inplace=True)\n",
        "#   #df = df.sample(n=25000, random_state=42)\n",
        "#   # Calculate number of words in each line of 2 columns\n",
        "#   df['len_eng_words'] = df['English'].apply(lambda e: len(str(e).split(\" \")))\n",
        "#   df['len_hin_words'] = df['Hindi'].apply(lambda h: len(str(h).split(\" \")))\n",
        "#   df = df[df['len_eng_words']<=20]\n",
        "#   df = df[df['len_hin_words']<=20]\n",
        "  \n",
        "#   return df"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-APCa11ScO3E"
      },
      "source": [
        "# def unicode_to_ascii(s):\n",
        "#     return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# def preprocessing_data_hin(w):\n",
        "#     w = unicode_to_ascii(str(w).lower().strip())\n",
        "#     w = re.sub(r\"[\\(\\[].*?[\\)\\]]\", ' ', w)\n",
        "#     w = re.sub(r\"([.?!,])\", r' \\1 ', w)\n",
        "#     w = re.sub(r'[\"“”‘]', \" \", w)\n",
        "#     w = re.sub(r'[0-9]', \" \", w)\n",
        "    \n",
        "#     w = w.strip()\n",
        "\n",
        "#     #w = '<start> ' + w + ' <end>'\n",
        "#     return w\n",
        "\n",
        "# def preprocessing_data_eng(w):\n",
        "#     w = unicode_to_ascii(str(w).lower().strip())\n",
        "#     w = re.sub(r\"[\\(\\[].*?[\\)\\]]\", '', w)\n",
        "#     w = re.sub(r\"([.?!,])\", r' \\1 ', w)\n",
        "#     w = re.sub(r'[\"]', \" \", w)\n",
        "#     w = re.sub(r'[^a-zA-Z?.!,]+', \" \", w)\n",
        "\n",
        "#     w = w.strip()\n",
        "\n",
        "#     return w"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytHCv2k1Uu5E"
      },
      "source": [
        "# def customize_dataset(df):\n",
        "#     df['Hindi'] = df['Hindi'].apply(lambda h : preprocessing_data_hin(h))\n",
        "#     df['English'] = df['English'].apply(lambda e : preprocessing_data_eng(e))\n",
        "#     df['Hindi'] = df['Hindi'].apply(lambda h: ''.join(c for c in h if c not in punctuations))\n",
        "#     df['English'] = df['English'].apply(lambda e: ''.join(c for c in e if c not in punctuations))\n",
        "#     df['English']=df['English'].apply(lambda e: e.translate(digits_rm))\n",
        "#     df['Hindi']=df['Hindi'].apply(lambda h: h.translate(digits_rm))\n",
        "#     df['Hindi']=df['Hindi'].apply(lambda h: re.sub(\"[a-z२३०८१५७९४६]\", '', h))\n",
        "#     df['English'] = df['English'].apply(lambda e : '<start> ' + e + ' <end>')\n",
        "#     df['Hindi'] = df['Hindi'].apply(lambda h : '<start> ' + h + ' <end>')\n",
        "\n",
        "#     return df['English'], df['Hindi']"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLpPTOmtSIfZ"
      },
      "source": [
        "# def tokenizing(sen):\n",
        "#     words = []\n",
        "#     for s in sen:\n",
        "#         for w in s.split():\n",
        "#             if w not in words:\n",
        "#                 words.append(w)\n",
        "#     return words"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1ad12m00aV_"
      },
      "source": [
        "# def load_dataset():\n",
        "#     df = create_dataset('https://raw.githubusercontent.com/prismspeechproject/neural/master/implementation/English_hindi.csv')\n",
        "\n",
        "#     input_lang, target_lang = customize_dataset(df)\n",
        "\n",
        "#     eng_words = sorted(tokenizing(input_lang))\n",
        "#     hin_words = sorted(tokenizing(target_lang))\n",
        "\n",
        "#     return input_lang, target_lang, eng_words, hin_words, df"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfx6cz3a1UTO",
        "outputId": "ef5c083a-5314-4161-ae0c-42f4990aed6e"
      },
      "source": [
        "# input_lang, target_lang, eng_words, hin_words, df = load_dataset()\n",
        "\n",
        "# num_encoder_words, num_decoder_words, max_input_sen_len, max_target_sen_len = len(eng_words) + 1, len(hin_words) + 1, max(df['len_eng_words']), max(df['len_hin_words'])\n",
        "'''\n",
        "        Here I loaded imported prepocessdata.load_dataset\n",
        "'''\n",
        "\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/prismspeechproject/neural/master/implementation/English_hindi.csv'\n",
        "input_dict_token, target_dict_token, reverse_input_dict_token, reverse_target_dict_token, df = pe.load_dataset(url)\n",
        "num_encoder_words  = len(input_dict_token) + 1\n",
        "num_decoder_words = len(target_dict_token) + 1\n",
        "max_input_sen_len = max(df['len_eng_words'])\n",
        "max_target_sen_len = max(df['len_hin_words'])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (7,8,9,10,11,12,13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC2NpRB31VJ1"
      },
      "source": [
        "# input_dict_token = dict([(word, i+1) for i, word in enumerate(eng_words)])\n",
        "# target_dict_token = dict([(word, i+1) for i, word in enumerate(hin_words)])\n",
        "# reverse_input_dict_token = dict([(i, word) for i, word in enumerate(eng_words)])\n",
        "# reverse_target_dict_token = dict([(i, word) for i, word in enumerate(hin_words)])\n",
        "# df = shuffle(df)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-v92Xez1YJX",
        "outputId": "b7f2eb8a-2624-4c21-ad9c-2a699f55b5da"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df['English'], df['Hindi'], test_size=0.2 , random_state=42)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((99605,), (24902,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17wVkWau1axr"
      },
      "source": [
        "def generate_batch(X, Y, batch_size=1):\n",
        "\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_input_sen_len), dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_target_sen_len), dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_target_sen_len, num_decoder_words), dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j + batch_size], Y[j: j+batch_size])):\n",
        "                for t, w in enumerate(input_text.split()):\n",
        "                    encoder_input_data[i, t] = input_dict_token[w]\n",
        "                for t, w in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_dict_token[w]\n",
        "                    if t > 0:\n",
        "                        decoder_target_data[i, t-1, target_dict_token[w]] = 1\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaIIftsOeeMX"
      },
      "source": [
        "\n",
        "latent_dim = 300\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(num_encoder_words, latent_dim, mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m3kgsYaefbo"
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_words, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_words, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDUNd0VQh5e5"
      },
      "source": [
        "# Encode the input sequence to get the \"thought vectors\"\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV0jZMGb1c6N"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_dict_token['<start>']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_dict_token[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '<end>' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSIY3X-K5qYM"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/Colab/CopyUntitled4/nmt_weights22Ep100.h5')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4pnCUh86qdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f248cf-b329-47b9-d33c-21345adcdaa9"
      },
      "source": [
        "model.summary"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.summary of <keras.engine.functional.Functional object at 0x7f26f6b22910>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo8bZ9eOXLG_"
      },
      "source": [
        "wordLen3 = pd.read_csv('https://raw.githubusercontent.com/prismspeechproject/neural/colab-engines/implementation/TestDataset/WordLen3.csv')\n",
        "wordLen3 = shuffle(wordLen3)\n",
        "wordLen3X , wordLen3Y = wordLen3['English'], wordLen3['Hindi']"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSg7a3WYPm2I"
      },
      "source": [
        "test_gen = generate_batch(wordLen3X, wordLen3Y, batch_size=1)\n",
        "k =-1"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psw1IpbUc2Kj",
        "outputId": "429a82ea-48ac-4bb2-d6a9-490e5577bbfe"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(test_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', wordLen3X[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', wordLen3Y[k:k+1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: <start> year old children <end>\n",
            "Actual Hindi Translation: > तमिल बोलन वाल , <\n",
            "Predicted Hindi Translation:  विधिवतता कठार सरटिफिकशन निरनतर बालको अथॉरीटी उ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuImDsZSc3e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6362c883-0621-4d14-99b6-c54ce827b6f4"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(test_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', wordLen3X[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', wordLen3Y[k:k+1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: <start> of the system . <end>\n",
            "Actual Hindi Translation: > जाचना चाहता था . <\n",
            "Predicted Hindi Translation:  जामाडोबा विचितरता पनिसलर कठपतलील कठपतलील कठपतलील त\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aEzOGl8up03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb9531d-22e0-4773-b387-9690745e7f91"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(test_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', wordLen3X[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', wordLen3Y[k:k+1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: <start> at stanford involved . <end>\n",
            "Actual Hindi Translation: > सटनफोरड क ऊषमपरवगिकी विभाग को शामिल किया . <\n",
            "Predicted Hindi Translation:  तलाशगा सजञाय डिजाइन चशम शिकसत तलल गजा रियायत दलबदलवि\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YkO2_GButCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3d8712-9be4-40f1-8067-117e84f94976"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(test_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', wordLen3X[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', wordLen3Y[k:k+1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: <start> struggled with school . <end>\n",
            "Actual Hindi Translation: > उन सब को सकली पढाई म दिककत हई थी । <\n",
            "Predicted Hindi Translation:  तलाशगा कीऋए सशलिषट भगनावशिषट इनहोन मिडिया करमयोग ब\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv3myr2V0CiK"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v880ei58uyUv",
        "outputId": "9ecffc70-7654-4ff0-ab5c-6cd173381d58"
      },
      "source": [
        "print('BLEU-1: %f' % corpus_bleu([actual_output], [decoded_sentence], weights=(1.0, 0, 0, 0)))\n",
        "print('BLEU-2: %f' % corpus_bleu([actual_output], [decoded_sentence], weights=(0.5, 0.5, 0, 0)))\n",
        "print('BLEU-3: %f' % corpus_bleu([actual_output], [decoded_sentence], weights=(0.33, 0.33, 0.33, 0)))\n",
        "print('BLEU-4: %f' % corpus_bleu([actual_output], [decoded_sentence], weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.000000\n",
            "BLEU-2: 0.000000\n",
            "BLEU-3: 0.000000\n",
            "BLEU-4: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0pEzs0Bz_5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6ff9bb-fba7-45b1-c2d8-6ae627621570"
      },
      "source": [
        "print('Individual 1-gram: %f' % corpus_bleu([actual_output], [decoded_sentence], weights=(1, 0, 0, 0)))\n",
        "print('Individual 2-gram: %f' % corpus_bleu([actual_output], [decoded_sentence], weights=(0, 1, 0, 0)))\n",
        "print('Individual 3-gram: %f' % corpus_bleu([actual_output], [decoded_sentence], weights=(0, 0, 1, 0)))\n",
        "print('Individual 4-gram: %f' % corpus_bleu([actual_output], [decoded_sentence], weights=(0, 0, 0, 1)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Individual 1-gram: 0.000000\n",
            "Individual 2-gram: 0.000000\n",
            "Individual 3-gram: 0.000000\n",
            "Individual 4-gram: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}